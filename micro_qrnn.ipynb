{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from functions.preprocess import input_shaping, split_index\n",
    "from functions.metrics import compute_rmse, compute_pearson\n",
    "from functions.channel_mapping import channel_mapping\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from tensorflow.keras.layers import Dense, Activation, Lambda , Input , Flatten ,Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from tensorflow import random\n",
    "import time as timer\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, SpatialDropout1D\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from functions.QRNN import QRNN\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "# import torch\n",
    "# from torchinfo import summary\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation\n",
      "Loading input features from file: spike_data/features/indy_20160627_01_spike_features_50ms.h5\n",
      "Loading kinematic data from file: kinematic_data/indy_20160627_01_kinematic_data.h5\n",
      "Hyperparameters >> units=150, epochs=6, batch_size=64, dropout=0.0, lrate=0.0035\n",
      "100\n",
      "2\n",
      "Formatting input feature data\n",
      "Formatting output (kinematic) data\n",
      "Splitting input dataset into training, validation, and testing subdataset\n",
      "Instantiating and training model...\n",
      "(672552, 1, 100)\n",
      "(672552, 2)\n"
     ]
    }
   ],
   "source": [
    "print (\"Starting simulation\")\n",
    "run_start = timer.time()\n",
    "seed = 2020\n",
    "feature_list = ['sua_rate', 'mua_rate']\n",
    "feature = feature_list[1] # select which spike feature: SUA=0, MUA=1\n",
    "\n",
    "# specify filename to be processed (choose from the list available at https://zenodo.org/record/583331)\n",
    "file_name = 'indy_20160627_01'          # file name\n",
    "# file_name = 'indy_20170131_02'          # file name\n",
    "kinematic_folder = 'kinematic_data/'    # kinematic data folder\n",
    "feature_folder = 'spike_data/features/' # spike features folder\n",
    "result_folder = 'results/'              # results folder\n",
    "\n",
    "wdw_time = 0.05 # window size in second\n",
    "lag = -32 # lag between kinematic and feature data (minus indicate feature lagging behaind kinematic)\n",
    "delta_time = 0.004 # sampling interval in second\n",
    "wdw_samp = int(round(wdw_time/delta_time))\n",
    "ol_samp = wdw_samp-1\n",
    "\n",
    "# open spike features from hdf5 file\n",
    "feature_file = feature_folder+file_name+'_spike_features_'+str(int(wdw_time*1e3))+'ms.h5'\n",
    "print (\"Loading input features from file: \"+feature_file)\n",
    "channel_mapping_file = r'F:/dropbox/Dropbox (Imperial NGNI)/NGNI Share/Workspace/Zheng/Research_Topics/signal processing plantform/prediction/decoding/raw_data/indy_20170127_03.nwb'\n",
    "\n",
    "with h5py.File(feature_file,'r') as f:\n",
    "    input_feature = f[feature][()]\n",
    "    # with h5py.File(channel_mapping_file, \"r\") as f:\n",
    "        # channel_loc =f['/general/extracellular_ephys/electrode_map'][()]\n",
    "        # input_feature = channel_mapping(input_feature,channel_loc)\n",
    "    input_feature = input_feature.reshape(input_feature.shape[0],-1)\n",
    "# open kinematic data from hdf5 file\n",
    "kinematic_file = kinematic_folder+file_name+'_kinematic_data.h5'\n",
    "print (\"Loading kinematic data from file: \"+kinematic_file)\n",
    "with h5py.File(kinematic_file,'r') as f:\n",
    "    cursor_vel = f['cursor_vel'][()] # in mm/s\n",
    "#%%\n",
    "# set LSTM hyperparameters\n",
    "units = 150 # SUA: 200, MUA: 150\n",
    "epochs = 6\n",
    "batch_size = 64\n",
    "dropout = 0.\n",
    "lrate = 0.0035 # SUA: 0.002, MUA: 0.0035\n",
    "print(\"Hyperparameters >> units={}, epochs={}, batch_size={}, dropout={:.1f}, lrate={:.4f}\".format(\n",
    "    units, epochs, batch_size, dropout, lrate))          \n",
    "\n",
    "# Define dictionary of parameters    \n",
    "num_layers = 1 # number of layers\n",
    "optimizer = 'RMSprop' # optimizer\n",
    "timesteps = 1 # number of timesteps (lag + current)\n",
    "input_dim = input_feature.shape[1] # input dimension\n",
    "print(input_dim)\n",
    "output_dim = cursor_vel.shape[1] # output dimension\n",
    "print(output_dim)\n",
    "verbose = 0\n",
    "\n",
    "load_name = result_folder+file_name+'model.h5'\n",
    "save_name = result_folder+file_name+'model.h5'\n",
    "\n",
    "params = {'num_layers':num_layers,'units':units, 'epochs':epochs, 'batch_size':batch_size, 'dropout':dropout, 'lrate':lrate,\n",
    "          'timesteps':timesteps,'input_dim':input_dim,'output_dim':output_dim,'seed':seed,'fit_gen':False,\n",
    "          'optimizer':optimizer, 'stateful':True, 'shuffle':True, 'verbose':verbose, 'load':False,\n",
    "          'load_name':load_name,'save':False, 'save_name':save_name, 'retrain':True} \n",
    "\n",
    "num_fold = 10 # number of folds\n",
    "\n",
    "# initialise performance scores (RMSE and CC) with nan values\n",
    "loss_train = np.full((num_fold, epochs), np.nan)\n",
    "loss_valid = np.copy(loss_train)\n",
    "rmse_valid = np.full((num_fold, output_dim), np.nan)\n",
    "rmse_test = np.copy(rmse_valid)\n",
    "cc_valid = np.copy(rmse_valid)\n",
    "cc_test = np.copy(rmse_valid)\n",
    "time_train = np.full((num_fold), np.nan)\n",
    "time_test = np.copy(time_train) \n",
    "\n",
    "print (\"Formatting input feature data\")\n",
    "tstep = timesteps # timestep (lag + current) samples\n",
    "stride = 1 # number of samples to be skipped\n",
    "X_in = input_shaping(input_feature, tstep, stride)\n",
    "\n",
    "print (\"Formatting output (kinematic) data\")\n",
    "diff_samp = cursor_vel.shape[0]-X_in.shape[0]\n",
    "Y_out = cursor_vel[diff_samp:,:] # in mm/s (remove it for new corrected velocity)\n",
    "\n",
    "print (\"Splitting input dataset into training, validation, and testing subdataset\")\n",
    "all_train_idx, all_valid_idx, all_test_idx = split_index(Y_out, num_fold)\n",
    "\n",
    "for i in range(num_fold):    \n",
    "    train_idx = all_train_idx[i]\n",
    "    valid_idx = all_valid_idx[i]\n",
    "    test_idx = all_test_idx[i]\n",
    "    \n",
    "    # specify training dataset\n",
    "    X_train = X_in[train_idx,:]            \n",
    "    Y_train = Y_out[train_idx,:]\n",
    "    \n",
    "    # specify validation dataset\n",
    "    X_valid = X_in[valid_idx,:]\n",
    "    Y_valid = Y_out[valid_idx,:]\n",
    "    \n",
    "    # specify validation dataset\n",
    "    X_test = X_in[test_idx,:]\n",
    "    Y_test = Y_out[test_idx,:]\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    # Standardize (z-score) input dataset\n",
    "    X_train_mean = np.nanmean(X_train, axis=0)\n",
    "    X_train_std = np.nanstd(X_train, axis=0) \n",
    "    X_train = (X_train - X_train_mean)/(X_train_std+epsilon)\n",
    "    X_valid = (X_valid - X_train_mean)/(X_train_std +epsilon)\n",
    "    X_test = (X_test - X_train_mean)/(X_train_std +epsilon)\n",
    "    \n",
    "    # Zero mean (centering) output dataset\n",
    "    Y_train_mean = np.nanmean(Y_train, axis=0)\n",
    "    Y_train_std = np.nanstd(Y_train, axis=0) \n",
    "    Y_train = (Y_train - Y_train_mean)/(Y_train_std+epsilon)\n",
    "    Y_valid = (Y_valid - Y_train_mean)/(Y_train_std +epsilon)\n",
    "    Y_test = (Y_test - Y_train_mean)/(Y_train_std +epsilon)\n",
    "           \n",
    "    #Re-align data to take lag into account\n",
    "    if lag < 0:\n",
    "        X_train = X_train[:lag,:] # remove lag first from end (X lag behind Y)\n",
    "        Y_train = Y_train[-lag:,:] # reomve lag first from beginning\n",
    "        X_valid = X_valid[:lag,:]\n",
    "        Y_valid = Y_valid[-lag:,:]\n",
    "        X_test = X_test[:lag,:]\n",
    "        Y_test = Y_test[-lag:,:]\n",
    "    if lag > 0:\n",
    "        X_train = X_train[lag:,:] # reomve lag first from beginning\n",
    "        Y_train = Y_train[:-lag,:] # remove lag first from end (X lead in front of Y)\n",
    "        X_valid = X_valid[lag:,:]\n",
    "        Y_valid = Y_valid[:-lag,:]            \n",
    "        X_test = X_test[lag:,:]\n",
    "        Y_test = Y_test[:-lag,:]\n",
    "        \n",
    "    # set seed to get reproducible results\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "print(\"Instantiating and training model...\")  \n",
    "# X_train = np.moveaxis(X_train,1,-1)\n",
    "# X_valid = np.moveaxis(X_valid,1,-1)\n",
    "# X_test = np.moveaxis(X_test,1,-1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexo\\anaconda3\\envs\\tf26\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4211: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21018/21018 [==============================] - 256s 12ms/step - loss: 0.4215 - mse: 0.4134\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "qrnn (QRNN)                  (None, 32)                9696      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 9,762\n",
      "Trainable params: 9,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "model.add(QRNN(32, window_size=1, dropout=0.2, \n",
    "               kernel_regularizer=l2(1e-4), bias_regularizer=l2(1e-4), \n",
    "               kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss = \"mean_squared_error\",  optimizer = \"adam\", metrics = ['mse'])\n",
    "model.fit(X_train, Y_train, epochs=1, batch_size=batch_size, verbose=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\alexo\\AppData\\Local\\Temp\\tmp0gmukq5c\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "model.save('qrnn_pls.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.representative_dataset = representative_dataset\n",
    "# # Ensure that if any ops can't be quantized, the converter throws an error\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# # Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "# converter.inference_input_type = tf.uint8\n",
    "# converter.inference_output_type = tf.uint8\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tflite_model = converter.convert()\n",
    "with open('qrnn.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fcac6d1b7b49b6af56953ca51a0c33c0fb424b1ca2308f3c571de63999461ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
