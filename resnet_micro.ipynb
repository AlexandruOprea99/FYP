{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from functions.preprocess import input_shaping, split_index\n",
    "# from functions.decoders import CNNDecoder\n",
    "from functions.metrics import compute_rmse, compute_pearson\n",
    "from functions.channel_mapping import channel_mapping\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "#from tensorflow.keras.layers import Dense, Activation, Lambda , Input , Flatten ,Conv2D, MaxPooling2D, Dropout \n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization,MaxPooling2D,Dropout, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D,Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from functions.metrics import compute_rmse, compute_pearson, pearson_r\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import np_utils\n",
    "from tensorflow import random\n",
    "import time as timer\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print(tf.executing_eagerly())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorflow version is:\n",
      "2.6.0\n",
      "If the version is > 2.6 STMCubeAI import from keras will return an error, please then use the tflite version\n",
      "Num GPUs Available:  1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print('The tensorflow version is:')\n",
    "print(tf.__version__)\n",
    "print('If the version is > 2.6 STMCubeAI import from keras will return an error, please then use the tflite version')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "   \n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "    \n",
    "    X = Add()([X, X_shortcut])# SKIP Connection\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def custom_residual_block(X,feature,block='0',stride=[],kernel=3,identity=True,batch_norm_end=False):\n",
    "    conv_name_base = 'res'  + block + '_branch'\n",
    "    bn_name_base = 'bn' + block + '_branch'\n",
    "    X_shortcut = X\n",
    "    no_stride=False\n",
    "    if not stride:\n",
    "        stride = [1]*len(feature)\n",
    "        no_stride = True\n",
    "        \n",
    "    # if no_stride:\n",
    "    #     pad='same'\n",
    "    # else:\n",
    "    #     pad='valid' \n",
    "\n",
    "    for i in range(0,len(feature)):\n",
    "\n",
    "        if i == 0:\n",
    "            X = Conv2D(filters=feature[i], kernel_size=(kernel, kernel), strides=(stride[i], stride[i]),kernel_initializer=glorot_uniform(seed=0), padding='same', name=conv_name_base + str(i))(X)\n",
    "        else:\n",
    "            X = Conv2D(filters=feature[i], kernel_size=(kernel, kernel), strides=(1,1), padding='same',kernel_initializer=glorot_uniform(seed=0), name=conv_name_base + str(i))(X)    \n",
    "        if not batch_norm_end:\n",
    "            X=BatchNormalization(axis=3, name=bn_name_base + str(i))(X)\n",
    "\n",
    "    if not identity:\n",
    "        output = X.shape\n",
    "        output = output[-2:]\n",
    "        output = tf.convert_to_tensor(output)\n",
    "        proto_tensor = tf.make_tensor_proto(output) \n",
    "        output = tf.make_ndarray(proto_tensor) \n",
    "        input = X_shortcut.shape\n",
    "        input = input[-2:]\n",
    "        input = tf.convert_to_tensor(input)\n",
    "        proto_tensor = tf.make_tensor_proto(input)  \n",
    "        input = tf.make_ndarray(proto_tensor) \n",
    "        if output[-1]==input[-1] and output[-2]==input[-2]:  \n",
    "            X_shortcut = Conv2D(filters=feature[-1], kernel_size=(kernel, kernel), strides=(1, 1), padding='same',kernel_initializer=glorot_uniform(seed=0), name=conv_name_base + 'short')(X)\n",
    "        else:\n",
    "            \n",
    "            kernel_eq = input[-2] - ((output[-2]-1)*stride[0])\n",
    "            X_shortcut = Conv2D(filters=feature[-1], kernel_size=(kernel_eq, kernel_eq), strides=(stride[0], stride[0]),kernel_initializer=glorot_uniform(seed=0), padding='same', name=conv_name_base + 'short')(X)\n",
    "        if not batch_norm_end:\n",
    "            X_shortcut = BatchNormalization(axis=3,name=bn_name_base + 'short')(X_shortcut)\n",
    "    #X = Add()([X, X_shortcut])\n",
    "    if batch_norm_end:\n",
    "        X = BatchNormalization(axis=3,name=bn_name_base + 'final')(X)\n",
    "    X = Activation('relu')(X)                       \n",
    "\n",
    "    return X\n",
    "\n",
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "   \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "    \n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def ResNet50_micro(input_shape=(10, 10, 1)):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    #X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    X = Conv2D(64, (2, 2), strides=(1, 1),padding='same', name='conv1')(X_input)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    #X = MaxPooling2D((3, 3), strides=(1, 1))(X)\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[4, 4, 16], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [4, 4, 16], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [4, 4, 16], stage=2, block='c')\n",
    "\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[8, 8, 32], stage=3, block='a', s=1)\n",
    "    X = identity_block(X, 3, [8, 8, 32], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [8, 8, 32], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [8, 8, 32], stage=3, block='d')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters = [16, 16, 64], stage=4, block='a', s=1)\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=4, block='f')\n",
    "\n",
    "    X = X = convolutional_block(X, f=3, filters=[32, 32, 128], stage=5, block='a', s=1)\n",
    "    X = identity_block(X, 3, filters=[32, 32, 128], stage=5, block='b')\n",
    "    X = identity_block(X, 3, filters=[32, 32, 128], stage=5, block='c')\n",
    "\n",
    "    X = AveragePooling2D(pool_size=(2, 2))(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(100)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(10)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(2)(X)\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "\n",
    "    return model\n",
    "\n",
    "def custom_ResNet_micro(featurses,strides=[],input_shape=(10, 10, 1),name='custom_resnet'):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "    X = Conv2D(16,kernel_size= (3, 3), strides=(1, 1),padding='same',kernel_initializer=glorot_uniform(seed=0), name='conv1')(X_input)\n",
    "    #X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    for i in range(len(featurses)):\n",
    "        if i == 0:\n",
    "            X= custom_residual_block(X,feature=featurses[i], block=str(i))\n",
    "        else:        \n",
    "            X= custom_residual_block(X,feature=featurses[i],block=str(i))\n",
    "\n",
    "    X = AveragePooling2D(pool_size=(2, 2))(X)\n",
    "    X = Flatten()(X)\n",
    "    # X = Dense(50,activation='relu',kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # X = Dropout(0.30)(X)\n",
    "    X = Dense(5,kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dropout(0.10)(X)\n",
    "    X = Dense(2,kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    model = Model(inputs=X_input, outputs=X, name=name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation\n",
      "Loading input features from file: spike_data/features/indy_20160915_01_spike_features_128ms.h5\n",
      "Loading kinematic data from file: kinematic_data/indy_20160915_01_kinematic_data.h5\n",
      "Hyperparameters >> units=300, window_size=2, epochs=32, batch_size=64, dropout=0.0, lrate=0.0003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 100 # random seed for reproducibility\n",
    "\n",
    "print (\"Starting simulation\")\n",
    "run_start = timer.time()\n",
    "\n",
    "feature_list = ['sua_rate', 'mua_rate']\n",
    "feature = feature_list[1] # select which spike feature: SUA=0, MUA=1\n",
    "\n",
    "# specify filename to be processed (choose from the list available at https://zenodo.org/record/583331)\n",
    "file_name = 'indy_20160915_01'          # file name\n",
    "kinematic_folder = 'kinematic_data/'    # kinematic data folder\n",
    "feature_folder = 'spike_data/features/' # spike features folder\n",
    "result_folder = 'results/'              # results folder\n",
    "wdw_time = 0.128 # window size in second\n",
    "lag = -32 # lag between kinematic and feature data (minus indicate feature lagging behaind kinematic)\n",
    "delta_time = 0.004 # sampling interval in second\n",
    "wdw_samp = int(round(wdw_time/delta_time))\n",
    "ol_samp = wdw_samp-1\n",
    "\n",
    "# open spike features from hdf5 file\n",
    "feature_file = feature_folder+file_name+'_spike_features_'+str(int(wdw_time*1e3))+'ms.h5'\n",
    "print (\"Loading input features from file: \"+feature_file)\n",
    "with h5py.File(feature_file,'r') as f:\n",
    "    input_feature = f[feature][()]\n",
    "channel_mapping_file ='raw_data/indy_20160915_01.nwb' #r'F:/dropbox/Dropbox (Imperial NGNI)/NGNI Share/Workspace/Zheng/Research_Topics/signal processing plantform/prediction/decoding/raw_data/indy_20170127_03.nwb'\n",
    "\n",
    "with h5py.File(channel_mapping_file, \"r\") as f:\n",
    "    channel_loc = f['/general/extracellular_ephys/electrode_map'][()]\n",
    "input_feature = channel_mapping(input_feature,channel_loc)\n",
    "# open kinematic data from hdf5 file\n",
    "kinematic_file = kinematic_folder+file_name+'_kinematic_data.h5'\n",
    "print (\"Loading kinematic data from file: \"+kinematic_file)\n",
    "with h5py.File(kinematic_file,'r') as f:\n",
    "    cursor_vel = f['cursor_vel'][()] # in mm/s\n",
    "\n",
    "# set QRNN hyperparameters\n",
    "units = 300 # SUA: 200, MUA: 150\n",
    "window_size = 2\n",
    "epochs = 32\n",
    "batch_size = 64\n",
    "dropout = 0.\n",
    "lrate = 0.0003\n",
    "\n",
    "num_fold = 10 # number of folds\n",
    " # SUA: 0.002, MUA: 0.0035 \n",
    "weight_decay = 1e-4\n",
    "print(\"Hyperparameters >> units={}, window_size={}, epochs={}, batch_size={}, dropout={:.1f}, lrate={:.4f}\".format(\n",
    "    units, window_size, epochs, batch_size, dropout, lrate))          \n",
    "\n",
    "# Define dictionary of parameters    \n",
    "num_layers = 1 # number of layers\n",
    "optimizer = 'adam' # optimizer\n",
    "timesteps = 1 # number of timesteps (lag + current)\n",
    "\n",
    "input_dim = timesteps#input_feature.shape[1] # input dimension\n",
    "output_dim = cursor_vel.shape[1] # output dimension\n",
    "verbose = 0\n",
    "\n",
    "setting1 = 2\n",
    "setting2 =3\n",
    "# in_planes = {1:4,2:8,3:16,4:32}\n",
    "\n",
    "featureses = {0:[16,16]}#,1:[16,16]}#,2:[16,16]} \n",
    "num_blockses = {1:[1],2:[1,1],3:[1,1,1],4:[2],5:[2,2],6:[2,2,2],7:[3],8:[3,3],9:[3,3,3]}\n",
    "\n",
    "features = [16,16,32,64]\n",
    "num_blocks = [3,3,3]\n",
    "model_name='resnet'\n",
    "save_path = result_folder+file_name+'_'+model_name+optimizer+'_batch_size_{}_'.format(batch_size)+'_lr_{}_'.format(lrate)+'_'+feature+'_'+str(int(wdw_time*1e3))+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting input feature data\n",
      "Formatting output (kinematic) data\n",
      "Splitting input dataset into training, validation, and testing subdataset\n",
      "Epoch 1/10\n",
      "1191/1191 [==============================] - 10s 8ms/step - loss: 0.6306 - pearson_r: -0.1832 - val_loss: 0.5257 - val_pearson_r: -0.3516\n",
      "Epoch 2/10\n",
      "1191/1191 [==============================] - 9s 7ms/step - loss: 0.5421 - pearson_r: -0.2199 - val_loss: 0.5058 - val_pearson_r: -0.3411\n",
      "Epoch 3/10\n",
      "1191/1191 [==============================] - 9s 8ms/step - loss: 0.5230 - pearson_r: -0.2287 - val_loss: 0.5145 - val_pearson_r: -0.3126\n",
      "Epoch 4/10\n",
      "1191/1191 [==============================] - 10s 8ms/step - loss: 0.5139 - pearson_r: -0.2291 - val_loss: 0.5276 - val_pearson_r: -0.3170\n",
      "Epoch 5/10\n",
      "1191/1191 [==============================] - 10s 8ms/step - loss: 0.4972 - pearson_r: -0.2281 - val_loss: 0.5437 - val_pearson_r: -0.3031\n",
      "Epoch 6/10\n",
      "1191/1191 [==============================] - 9s 7ms/step - loss: 0.4901 - pearson_r: -0.2296 - val_loss: 0.5339 - val_pearson_r: -0.2924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c2c018a820>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise performance scores (RMSE and CC) with nan values\n",
    "\n",
    "loss_train = np.full((num_fold, epochs), np.nan)\n",
    "loss_valid = np.copy(loss_train)\n",
    "rmse_valid = np.full((num_fold, output_dim), np.nan)\n",
    "rmse_test = np.copy(rmse_valid)\n",
    "cc_valid = np.copy(rmse_valid)\n",
    "cc_test = np.copy(rmse_valid)\n",
    "time_train = np.full((num_fold), np.nan)\n",
    "time_test = np.copy(time_train) \n",
    "\n",
    "print (\"Formatting input feature data\")\n",
    "tstep = timesteps # timestep (lag + current) samples\n",
    "stride = 1 # number of samples to be skipped\n",
    "X_in = input_shaping(input_feature.reshape(input_feature.shape[0],-1), timesteps, 1)\n",
    "X_in = X_in.reshape(X_in.shape[0],timesteps,10,10)\n",
    " \n",
    "print (\"Formatting output (kinematic) data\")\n",
    "diff_samp = cursor_vel.shape[0]-X_in.shape[0]\n",
    "Y_out = cursor_vel[diff_samp:,:] # in mm/s (remove it for new corrected velocity)\n",
    "\n",
    "print (\"Splitting input dataset into training, validation, and testing subdataset\")\n",
    "all_train_idx, all_valid_idx, all_test_idx = split_index(Y_out, num_fold)\n",
    "global temp\n",
    "for i in range(num_fold): \n",
    "    train_idx = all_train_idx[i]\n",
    "    valid_idx = all_valid_idx[i]\n",
    "    test_idx = all_test_idx[i]\n",
    "    \n",
    "    # specify training dataset\n",
    "    X_train = X_in[train_idx,:]            \n",
    "    Y_train = Y_out[train_idx,:]\n",
    "    \n",
    "    # specify validation dataset\n",
    "    X_valid = X_in[valid_idx,:]\n",
    "    Y_valid = Y_out[valid_idx,:]\n",
    "    \n",
    "    # specify validation dataset\n",
    "    X_test = X_in[test_idx,:]\n",
    "    Y_test = Y_out[test_idx,:]\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    # Standardize (z-score) input dataset\n",
    "    X_train_mean = np.nanmean(X_train, axis=0)\n",
    "    X_train_std = np.nanstd(X_train, axis=0) \n",
    "    X_train = (X_train - X_train_mean)/(X_train_std+epsilon)\n",
    "    X_valid = (X_valid - X_train_mean)/(X_train_std +epsilon)\n",
    "    X_test = (X_test - X_train_mean)/(X_train_std +epsilon)\n",
    "    \n",
    "    # Zero mean (centering) output dataset\n",
    "    Y_train_mean = np.nanmean(Y_train, axis=0)\n",
    "    Y_train_std = np.nanstd(Y_train, axis=0) \n",
    "    Y_train = (Y_train - Y_train_mean)/(Y_train_std+epsilon)\n",
    "    Y_valid = (Y_valid - Y_train_mean)/(Y_train_std +epsilon)\n",
    "    Y_test = (Y_test - Y_train_mean)/(Y_train_std +epsilon)\n",
    "    # Y_train = Y_train - Y_train_mean \n",
    "    # Y_valid = Y_valid - Y_train_mean\n",
    "    # Y_test = Y_test - Y_train_mean\n",
    "           \n",
    "    #Re-align data to take lag into accountc\n",
    "    if lag < 0:\n",
    "        X_train = X_train[:lag,:] # remove lag first from end (X lag behind Y)\n",
    "        Y_train = Y_train[-lag:,:] # reomve lag first from beginning\n",
    "        X_valid = X_valid[:lag,:]\n",
    "        Y_valid = Y_valid[-lag:,:]\n",
    "        X_test = X_test[:lag,:]\n",
    "        Y_test = Y_test[-lag:,:]\n",
    "    if lag > 0:\n",
    "        X_train = X_train[lag:,:] # reomve lag first from beginning\n",
    "        Y_train = Y_train[:-lag,:] # remove lag first from end (X lead in front of Y)\n",
    "        X_valid = X_valid[lag:,:]\n",
    "        Y_valid = Y_valid[:-lag,:]            \n",
    "        X_test = X_test[lag:,:]\n",
    "        Y_test = Y_test[:-lag,:]\n",
    "        \n",
    "    # set seed to get reproducible results\n",
    "    np.random.seed(seed)\n",
    "    random.set_seed(seed)\n",
    "    X_train = np.moveaxis(X_train,1,-1)\n",
    "    X_valid = np.moveaxis(X_valid,1,-1)\n",
    "    X_test = np.moveaxis(X_test,1,-1)\n",
    "\n",
    "# SMALL_SIZE = 9\n",
    "# MEDIUM_SIZE = 14\n",
    "# BIGGER_SIZE = 18\n",
    "\n",
    "# plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "# plt.rc('axes', titlesize=14)     # fontsize of the axes title\n",
    "# plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "# plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "# plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "# plt.rc('legend', fontsize=11)    # legend fontsize\n",
    "# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# N_rows=10\n",
    "# N_cols=10\n",
    "# start_val = 0 # pick an element for the code to plot the following N**2 values\n",
    "# fig, axes = plt.subplots(N_rows,N_cols)\n",
    "# fig.set_size_inches(15,19)\n",
    "# items = list(range(0, 10))\n",
    "# for row in range(N_rows):\n",
    "#   for col in range(N_cols):\n",
    "#     idx = start_val+col+N_rows*row\n",
    "#     axes[row,col].imshow(X_train[idx], cmap='viridis')\n",
    "#     #fig.subplots_adjust(hspace=0.25)\n",
    "#     axes[row,col].set_title('sample {}'.format(idx))\n",
    "#     axes[row,col].set_xticks([])\n",
    "#     axes[row,col].set_yticks([])\n",
    "# fig.savefig('images.jpg',format=\"jpg\",dpi=600, bbox_inches='tight', pad_inches=0.1)\n",
    "# fig_vel, axes_vel =plt.subplots(1)\n",
    "# fig_vel.set_size_inches(15,5)  \n",
    "# X = np.arange(0,100,1,dtype=int)   \n",
    "# axes_vel.xaxis.grid(True, linestyle='--')\n",
    "# axes_vel.yaxis.grid(True, linestyle='--')\n",
    "# axes_vel.plot(X,Y_train[0:100,0],linewidth=2,label=\"X axis velocity\")\n",
    "# axes_vel.plot(X,Y_train[0:100,1],linewidth=2,label=\"y axis velocity\")\n",
    "# axes_vel.set(xlabel='sample index',title='velocity graph')\n",
    "# axes_vel.legend(loc='upper right')\n",
    "# fig_vel.savefig('velocity.jpg',format=\"jpg\",dpi=600, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_pearson_r', patience=5,restore_best_weights=True)\n",
    "model = custom_ResNet_micro(featurses=featureses)\n",
    "#model = ResNet50_micro()\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "model.compile(loss='mse', optimizer=opt,metrics=[pearson_r])\n",
    "model.fit(X_train, Y_train,validation_data=(X_valid,Y_valid) ,epochs=10, batch_size=64,verbose=True,shuffle=True,callbacks=callback)\n",
    "\n",
    "\n",
    "\n",
    "# Y_valid_predict = model.predict(X_valid)\n",
    "# start = timer.time()\n",
    "# Y_test_predict = model.predict(X_test)\n",
    "# end = timer.time()\n",
    "\n",
    "\n",
    "# print(\"Model testing took {:.2f} seconds\".format(end - start)) \n",
    "# time_test[i] = end - start\n",
    "\n",
    "# # Compute performance metrics    \n",
    "# rmse_vld = compute_rmse(Y_valid, Y_valid_predict)\n",
    "# rmse_tst = compute_rmse(Y_test, Y_test_predict)\n",
    "# cc_vld = compute_pearson(Y_valid, Y_valid_predict)\n",
    "# cc_tst = compute_pearson(Y_test, Y_test_predict)\n",
    "\n",
    "# rmse_valid[i,:] = rmse_vld\n",
    "# rmse_test[i,:] = rmse_tst\n",
    "# cc_valid[i,:] = cc_vld\n",
    "# cc_test[i,:] = cc_tst\n",
    "# if os.path.exists(save_path) == False:\n",
    "#     os.makedirs(save_path)\n",
    "    \n",
    "# print(\"Fold-{} | Validation RMSE: {:.4f}\".format(i, np.nanmean(rmse_vld)))\n",
    "# print(\"Fold-{} | Validation CC: {:.4f}\".format(i, np.nanmean(cc_vld)))\n",
    "# print(\"Fold-{} | Testing RMSE: {:.4f}\".format(i, np.nanmean(rmse_tst)))\n",
    "# print(\"Fold-{} | Testing CC: {:.4f}\".format(i, np.nanmean(cc_tst)))\n",
    "\n",
    "# run_end = timer.time()\n",
    "# mean_rmse_valid = np.nanmean(rmse_valid, axis=0) \n",
    "# mean_rmse_test = np.nanmean(rmse_test, axis=0)\n",
    "# mean_cc_valid = np.nanmean(cc_valid, axis=0)\n",
    "# mean_cc_test = np.nanmean(cc_test, axis=0)\n",
    "# mean_time =  np.nanmean(time_train, axis=0)\n",
    "# print(\"----------------------------------------------------------------------\")\n",
    "# print(\"Validation Mean RMSE: %.5f \" %(np.mean(mean_rmse_valid)))\n",
    "# print(\"Validation Mean CC: %.5f \" %(np.mean(mean_cc_valid)))\n",
    "# print(\"Testing Mean RMSE: %.5f \" %(np.mean(mean_rmse_test)))\n",
    "# print(\"Testing Mean CC: %.5f \" %(np.mean(mean_cc_test)))\n",
    "# print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "# # storing evaluation results into hdf5 file\n",
    "# result_filename = result_folder+file_name+'_resnet50f10_'+feature+'_'+str(int(wdw_time*1e3))+'ms.h5'\n",
    "# print (\"Storing results into file: \"+result_filename)\n",
    "# with h5py.File(result_filename,'w') as f:\n",
    "#     f['Y_true'] = Y_test\n",
    "#     f['Y_predict'] = Y_test_predict\n",
    "#     f['rmse_valid'] = rmse_valid\n",
    "#     f['rmse_test'] = rmse_test\n",
    "#     f['cc_valid'] = cc_valid\n",
    "#     f['cc_test'] = cc_test\n",
    "#     f['time_train'] = time_train\n",
    "#     f['time_test'] = time_test\n",
    "\n",
    "# run_time = run_end - run_start\n",
    "# print (\"Finished whole processes within %.2f seconds\" % run_time)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25227815 -0.74588186]]\n"
     ]
    }
   ],
   "source": [
    "# model.summary()\n",
    "micro_valid = np.ones((1,10,10,1))\n",
    "output_micro = model.predict(micro_valid)\n",
    "print(output_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('cnnD.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.representative_dataset = representative_dataset\n",
    "# # Ensure that if any ops can't be quantized, the converter throws an error\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# # Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "# converter.inference_input_type = tf.uint8\n",
    "# converter.inference_output_type = tf.uint8\n",
    "tflite_model = converter.convert()\n",
    "with open('cnnD.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf26')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c25c14ff58167a79b0cdace4ba5d98aa0031a2e2994c0b04feed80f2230c95c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
