{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from functions.preprocess import input_shaping, split_index\n",
    "from functions.decoders import CNNDecoder\n",
    "from functions.metrics import compute_rmse, compute_pearson\n",
    "from functions.channel_mapping import channel_mapping\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers.recurrent import LSTM\n",
    "from tensorflow.keras.layers import Dense, Activation, Lambda , Input , Flatten ,Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from tensorflow import random\n",
    "import time as timer\n",
    "from keras.models import load_model\n",
    "from functions.metrics import compute_rmse, compute_pearson\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "# import torch\n",
    "# from torchinfo import summary\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation\n",
      "Loading input features from file: spike_data/features/indy_20160915_01_spike_features_128ms.h5\n",
      "Loading kinematic data from file: kinematic_data/indy_20160915_01_kinematic_data.h5\n",
      "Hyperparameters >> units=300, window_size=2, epochs=15, batch_size=32, dropout=0.0, lrate=0.0030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 2020 # random seed for reproducibility\n",
    "\n",
    "print (\"Starting simulation\")\n",
    "run_start = timer.time()\n",
    "\n",
    "feature_list = ['sua_rate', 'mua_rate']\n",
    "feature = feature_list[1] # select which spike feature: SUA=0, MUA=1\n",
    "\n",
    "# specify filename to be processed (choose from the list available at https://zenodo.org/record/583331)\n",
    "file_name = 'indy_20160915_01'          # file name\n",
    "kinematic_folder = 'kinematic_data/'    # kinematic data folder\n",
    "feature_folder = 'spike_data/features/' # spike features folder\n",
    "result_folder = 'results/'              # results folder\n",
    "wdw_time = 0.128 # window size in second\n",
    "lag = -32 # lag between kinematic and feature data (minus indicate feature lagging behaind kinematic)\n",
    "delta_time = 0.004 # sampling interval in second\n",
    "wdw_samp = int(round(wdw_time/delta_time))\n",
    "ol_samp = wdw_samp-1\n",
    "\n",
    "# open spike features from hdf5 file\n",
    "feature_file = feature_folder+file_name+'_spike_features_'+str(int(wdw_time*1e3))+'ms.h5'\n",
    "print (\"Loading input features from file: \"+feature_file)\n",
    "with h5py.File(feature_file,'r') as f:\n",
    "    input_feature = f[feature][()]\n",
    "channel_mapping_file ='raw_data/indy_20170127_03.nwb' #r'F:/dropbox/Dropbox (Imperial NGNI)/NGNI Share/Workspace/Zheng/Research_Topics/signal processing plantform/prediction/decoding/raw_data/indy_20170127_03.nwb'\n",
    "\n",
    "with h5py.File(channel_mapping_file, \"r\") as f:\n",
    "    channel_loc = f['/general/extracellular_ephys/electrode_map'][()]\n",
    "input_feature = channel_mapping(input_feature,channel_loc)\n",
    "# open kinematic data from hdf5 file\n",
    "kinematic_file = kinematic_folder+file_name+'_kinematic_data.h5'\n",
    "print (\"Loading kinematic data from file: \"+kinematic_file)\n",
    "with h5py.File(kinematic_file,'r') as f:\n",
    "    cursor_vel = f['cursor_vel'][()] # in mm/s\n",
    "\n",
    "# set QRNN hyperparameters\n",
    "units = 300 # SUA: 200, MUA: 150\n",
    "window_size = 2\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "dropout = 0.\n",
    "lrate = 0.003\n",
    "\n",
    "num_fold = 5 # number of folds\n",
    " # SUA: 0.002, MUA: 0.0035 \n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print(\"Hyperparameters >> units={}, window_size={}, epochs={}, batch_size={}, dropout={:.1f}, lrate={:.4f}\".format(\n",
    "    units, window_size, epochs, batch_size, dropout, lrate))          \n",
    "\n",
    "# Define dictionary of parameters    \n",
    "num_layers = 1 # number of layers\n",
    "optimizer = 'adam' # optimizer\n",
    "timesteps = 1 # number of timesteps (lag + current)\n",
    "kernel_init = initializers.glorot_uniform(seed=seed) \n",
    "recur_init =  initializers.Orthogonal(seed=seed)\n",
    "input_dim = timesteps#input_feature.shape[1] # input dimension\n",
    "output_dim = cursor_vel.shape[1] # output dimension\n",
    "verbose = 0\n",
    "\n",
    "setting1 = 2\n",
    "setting2 =3\n",
    "# in_planes = {1:4,2:8,3:16,4:32}\n",
    "\n",
    "featureses = {1:[4,4,8,16],2:[8,8,16,32],3:[16,16,32,64],4:[32,32,64,128]} \n",
    "num_blockses = {1:[1],2:[1,1],3:[1,1,1],4:[2],5:[2,2],6:[2,2,2],7:[3],8:[3,3],9:[3,3,3]}\n",
    "in_plane = [16]\n",
    "features_out = 64\n",
    "stride = [1,1,2,2]\n",
    "\n",
    "features = [16,16,32,64]\n",
    "num_blocks = [3,3,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting input feature data\n",
      "Formatting output (kinematic) data\n",
      "Splitting input dataset into training, validation, and testing subdataset\n",
      "(57124, 10, 10, 1)\n",
      "(57124, 2)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 32, 32, 1), dtype=tf.float32, name='input_7'), name='input_7', description=\"created by layer 'input_7'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='dense_36/BiasAdd:0', description=\"created by layer 'dense_36'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='model_6/dense_36/BiasAdd:0', description=\"created by layer 'model_6'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 10, 1), dtype=tf.float32, name='input_8'), name='input_8', description=\"created by layer 'input_8'\")\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 10, 10, 1)]       0         \n",
      "                                                                 \n",
      " lambda_3 (Lambda)           (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " model_6 (Functional)        (None, 2)                 23585538  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,585,538\n",
      "Trainable params: 23,532,418\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "1786/1786 [==============================] - 114s 61ms/step - loss: 13843.0352 - val_loss: 184170.6094\n",
      "Epoch 2/15\n",
      "1786/1786 [==============================] - 108s 60ms/step - loss: 15773.2979 - val_loss: 4625.0674\n",
      "Epoch 3/15\n",
      "1786/1786 [==============================] - 108s 60ms/step - loss: 8916.0039 - val_loss: 338560.6875\n",
      "Epoch 4/15\n",
      "1786/1786 [==============================] - 108s 60ms/step - loss: 4129.0771 - val_loss: 442708.1250\n",
      "Epoch 5/15\n",
      "1786/1786 [==============================] - 108s 60ms/step - loss: 5283.6201 - val_loss: 3832.9719\n",
      "Epoch 6/15\n",
      "1786/1786 [==============================] - 108s 61ms/step - loss: 11866.0840 - val_loss: 734303.6875\n",
      "Epoch 7/15\n",
      "1786/1786 [==============================] - 108s 61ms/step - loss: 15054.2734 - val_loss: 8343.0801\n",
      "Epoch 8/15\n",
      "1786/1786 [==============================] - 109s 61ms/step - loss: 16892.1738 - val_loss: 5735.9863\n",
      "Epoch 9/15\n",
      "1786/1786 [==============================] - 113s 63ms/step - loss: 7404.8594 - val_loss: 4442.6143\n",
      "Epoch 10/15\n",
      "1786/1786 [==============================] - 117s 65ms/step - loss: 3501.8293 - val_loss: 42698.8555\n",
      "Epoch 11/15\n",
      "1786/1786 [==============================] - 117s 66ms/step - loss: 2674.2771 - val_loss: 50764.4922\n",
      "Epoch 12/15\n",
      "1786/1786 [==============================] - 117s 66ms/step - loss: 3990.1250 - val_loss: 762241.7500\n",
      "Epoch 13/15\n",
      "1786/1786 [==============================] - 117s 66ms/step - loss: 10117.8975 - val_loss: 11686.9707\n",
      "Epoch 14/15\n",
      "1786/1786 [==============================] - 117s 65ms/step - loss: 15190.2168 - val_loss: 2824.7847\n",
      "Epoch 15/15\n",
      "1786/1786 [==============================] - 117s 66ms/step - loss: 5445.8477 - val_loss: 2373.8430\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 10, 10, 1)]       0         \n",
      "                                                                 \n",
      " lambda_3 (Lambda)           (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " model_6 (Functional)        (None, 2)                 23585538  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,585,538\n",
      "Trainable params: 23,532,418\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialise performance scores (RMSE and CC) with nan values\n",
    "loss_train = np.full((num_fold, epochs), np.nan)\n",
    "loss_valid = np.copy(loss_train)\n",
    "rmse_valid = np.full((num_fold, output_dim), np.nan)\n",
    "rmse_test = np.copy(rmse_valid)\n",
    "cc_valid = np.copy(rmse_valid)\n",
    "cc_test = np.copy(rmse_valid)\n",
    "time_train = np.full((num_fold), np.nan)\n",
    "time_test = np.copy(time_train) \n",
    "\n",
    "\n",
    "print (\"Formatting input feature data\")\n",
    "tstep = timesteps # timestep (lag + current) samples\n",
    "stride = 1 # number of samples to be skipped\n",
    "X_in = input_shaping(input_feature.reshape(input_feature.shape[0],-1), timesteps, 1)\n",
    "X_in = X_in.reshape(X_in.shape[0],timesteps,10,10)\n",
    "\n",
    "print (\"Formatting output (kinematic) data\")\n",
    "diff_samp = cursor_vel.shape[0]-X_in.shape[0]\n",
    "Y_out = cursor_vel[diff_samp:,:] # in mm/s (remove it for new corrected velocity)\n",
    "\n",
    "print (\"Splitting input dataset into training, validation, and testing subdataset\")\n",
    "all_train_idx, all_valid_idx, all_test_idx = split_index(Y_out, num_fold)\n",
    "global temp\n",
    "for i in range(num_fold): \n",
    "    train_idx = all_train_idx[i]\n",
    "    valid_idx = all_valid_idx[i]\n",
    "    test_idx = all_test_idx[i]\n",
    "    \n",
    "    # specify training dataset\n",
    "    X_train = X_in[train_idx,:]            \n",
    "    Y_train = Y_out[train_idx,:]\n",
    "    \n",
    "    # specify validation dataset\n",
    "    X_valid = X_in[valid_idx,:]\n",
    "    Y_valid = Y_out[valid_idx,:]\n",
    "    \n",
    "    # specify validation dataset\n",
    "    X_test = X_in[test_idx,:]\n",
    "    Y_test = Y_out[test_idx,:]\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    # Standardize (z-score) input dataset\n",
    "    X_train_mean = np.nanmean(X_train, axis=0)\n",
    "    X_train_std = np.nanstd(X_train, axis=0) \n",
    "    X_train = (X_train - X_train_mean)/(X_train_std+epsilon)\n",
    "    X_valid = (X_valid - X_train_mean)/(X_train_std +epsilon)\n",
    "    X_test = (X_test - X_train_mean)/(X_train_std +epsilon)\n",
    "    \n",
    "    # Zero mean (centering) output dataset\n",
    "    Y_train_mean = np.nanmean(Y_train, axis=0)\n",
    "    Y_train_std = np.nanstd(Y_train, axis=0) \n",
    "    Y_train = (Y_train - Y_train_mean)/(Y_train_std+epsilon)\n",
    "    Y_valid = (Y_valid - Y_train_mean)/(Y_train_std +epsilon)\n",
    "    Y_test = (Y_test - Y_train_mean)/(Y_train_std +epsilon)\n",
    "    # Y_train = Y_train - Y_train_mean \n",
    "    # Y_valid = Y_valid - Y_train_mean\n",
    "    # Y_test = Y_test - Y_train_mean\n",
    "           \n",
    "    #Re-align data to take lag into account\n",
    "    if lag < 0:\n",
    "        X_train = X_train[:lag,:] # remove lag first from end (X lag behind Y)\n",
    "        Y_train = Y_train[-lag:,:] # reomve lag first from beginning\n",
    "        X_valid = X_valid[:lag,:]\n",
    "        Y_valid = Y_valid[-lag:,:]\n",
    "        X_test = X_test[:lag,:]\n",
    "        Y_test = Y_test[-lag:,:]\n",
    "    if lag > 0:\n",
    "        X_train = X_train[lag:,:] # reomve lag first from beginning\n",
    "        Y_train = Y_train[:-lag,:] # remove lag first from end (X lead in front of Y)\n",
    "        X_valid = X_valid[lag:,:]\n",
    "        Y_valid = Y_valid[:-lag,:]            \n",
    "        X_test = X_test[lag:,:]\n",
    "        Y_test = Y_test[:-lag,:]\n",
    "        \n",
    "    # set seed to get reproducible results\n",
    "np.random.seed(seed)\n",
    "random.set_seed(seed)\n",
    "X_train = np.moveaxis(X_train,1,-1)\n",
    "X_valid = np.moveaxis(X_valid,1,-1)\n",
    "X_test = np.moveaxis(X_test,1,-1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "model_resnet = tf.keras.applications.ResNet50(include_top = False,input_shape=(32,32,1), weights=None)\n",
    "\n",
    "output = model_resnet.output\n",
    "output = Flatten()(output)\n",
    "output = Dense(units=2)(output)\n",
    "model = Model(model_resnet.input, output)\n",
    "print(model.input)\n",
    "print(model.output)\n",
    "inp = Input(shape=(10, 10, 1))\n",
    "out = Lambda(lambda image: tf.compat.v1.image.resize_images(image, (32, 32)))(inp)\n",
    "out = model(out)\n",
    "model = Model(inp, out)\n",
    "print(model.output)\n",
    "print(model.input)\n",
    "model.summary()\n",
    "\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), epochs=15, batch_size=32,verbose=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "      data = np.random.rand(1, 10, 10, 1)\n",
    "      yield [data.astype(np.float32)]\n",
    "\n",
    "\n",
    "model.save('my_model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.representative_dataset = representative_dataset\n",
    "# # Ensure that if any ops can't be quantized, the converter throws an error\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# # Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "# converter.inference_input_type = tf.uint8\n",
    "# converter.inference_output_type = tf.uint8\n",
    "tflite_model = converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea380406f12cb1fd65a6e3a9d9622de64e15d46f3292df5ccf8b9659c0986bbd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
